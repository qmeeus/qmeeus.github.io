---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I am Quentin, a dedicated machine learning researcher and engineer, passionate about pushing the boundaries of artificial intelligence and its applications. My academic background shaped my expertise in building and training deep neural networks for natural language processing (NLP) and spoken language understanding (SLU). 

Research Interests
======
My work primarily focus on low-resource spoken language understanding, natural language processing, and parameter-efficient finetuning techniques. I'm particularly drawn to the challenges of developing robust and accurate deep learning models in environments with limited data or computational resources, by researching methods to take the most advantage of available data. For example, I have researched how to train models with fewer data by learning multiple tasks simultaneously (multitask learning), learning from other languages with more resources (multilingual learning), and harvesting pretrained NLP models for low resource SLU research.
My goal is to contribute to the advancement of AI technology by developing innovative solutions that have a real-world impact.
Below is a non-exhaustive list of my contributions to the domain of artificial intelligence:
 - **Whisper-SLU** is a spoken language understanding model architecture for intent recognition and spoken named entity recognition. Two versions exist: v1 has a dedicated SLU module which clearly assign the parameters for each tasks (ASR or SLU). v2 is an integrated model which directly predicts annotated text from speech.
 - The **Bidirectional Representation Model for Speech** is a speech processing model that take advantage of bidirectional attention and masked language modeling to generate representations of speech which contain semantic information. These dense representations have a lower dimension and can be used efficiently to teach a downstream model to understand speech with few training examples.
 - **CLS Transformer for SLU** is a model architecture with fewer parameters than an equivalent Transformer, designed for low resource speech classification. Aside from its main benefit of requiring fewer training examples, it can also give insights to visually explain the model's predictions.
 - **MSNER** is a dataset for multilingual spoken named entity recognition. The training and validation sets are A.I. generated, but the test set is entirely annotated by hand, providing gold-quality labels.

Background and Education
======
I hold a PhD in Electrical Engineering and an Advanced Master in AI from KU Leuven, one of the leading universities in Europe. My academic journey has provided me with a deep understanding of the theoretical foundations and practical applications of these fields. Additionally, I have a Master in Business Engineering from the Solvay Brussels School of Economics and Management (ULB), which has given me valuable insights into the commercial aspects and real-world deployment of AI solutions.

Personal Life
======
When I'm not immersed in the world of AI, I enjoy spending my time outdoors. I'm a passionate mountain enthusiast and love rock climbing, trail running, and skiing. Music is another great passion of mine, and I enjoy exploring different genres and discovering new artists.


