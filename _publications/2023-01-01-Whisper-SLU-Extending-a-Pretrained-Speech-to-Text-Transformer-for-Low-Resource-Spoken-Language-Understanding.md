---
title: "Whisper-SLU: Extending a Pretrained Speech-to-Text Transformer for Low Resource Spoken Language Understanding"
collection: publications
category: conferences
permalink: /publication/2023-01-01-Whisper-SLU-Extending-a-Pretrained-Speech-to-Text-Transformer-for-Low-Resource-Spoken-Language-Understanding
date: 2023-01-01
venue: '2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)'
paperurl: '/files/Whisper_SLU.pdf'
excerpt: 'In recent years, transformers have become a fundamental part of machine learning research and have demonstrated impressive results in various fields. However, they require significant resources for training. This paper proposes extending the Whisper transformer-based model with dedicated modules for low resource spoken language understanding tasks.'
---

## Abstract
In recent years, transformers have become a fundamental part of machine learning research and have demonstrated impressive results in various fields. However, they require significant resources for training. This paper proposes extending the Whisper transformer-based model with dedicated modules for low resource spoken language understanding tasks, including named entity recognition and intent recognition. The authors borrow techniques from other fields of machine learning such as the BIO framework coupled with a conditional random field and class attention to improve how intent recognition is done in low resource environments. The proposed models demonstrate the benefits of multitask learning when fine-tuning Whisper for SLU with a limited number of labeled training examples. Overall, this paper contributes novel architectures for three low resource SLU tasks and explores the performance of pretraining and fine-tuning a speech-to-text transformer model.

[Download PDF](/files/Whisper_SLU.pdf)

[Online Access](https://ieeexplore.ieee.org/document/10389786){:target="_blank"}
