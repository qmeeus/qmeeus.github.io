---
title: "Multitask Learning for Low Resource Spoken Language Understanding"
collection: publications
category: conferences
permalink: /publication/2022-09-01-Multitask-Learning-for-Low-Resource-Spoken-Language-Understanding
date: 2022-09-01
venue: 'Interspeech 2022'
paperurl: '/files/MTL4LRSLU.pdf'
excerpt: 'We explore the benefits that multitask learning offer to speech processing as we train models on dual objectives with automatic speech recognition and intent classification or sentiment classification.'
---

## Abstract
We explore the benefits that multitask learning offer to speech processing as we train models on dual objectives with automatic speech recognition and intent classification or sentiment classification. Our models, although being of modest size, show improvements over models trained end-to-end on intent classification. We compare different settings to find the optimal disposition of each task module compared to one another. Finally, we study the performance of the models in low-resource scenario by training the models with as few as one example per class. We show that multitask learning in these scenarios compete with a baseline model trained on text features and performs considerably better than a pipeline model. On sentiment classification, we match the performance of an end-to-end model with ten times as many parameters. We consider 4 tasks and 4 datasets in Dutch and English.

[Download PDF](/files/MTL4LRSLU.pdf)

[Online Access](https://doi.org/10.21437%2Finterspeech.2022-11401){:target="_blank"}
